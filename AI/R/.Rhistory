x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
# vector of lambda, although it's not required because glmnet has a built-in range of lambda
grid = 10^seq(10, -2, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid) # alpha = 0 for Ridge
dim(coef(ridge.mod)) # Coef for each variable (20 rows) and lambda value (100 cols)
ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
sqrt(sum(coef(ridge.mod)[-1, 50]^2)) # l2 norm, 6.36
ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2)) # l2 norm, 57.1 much larger than before, aka more penalty
predict(ridge.mod, s = 50, type = "coefficients")[1:20,] # Ridge coeff for lambda = 50
## Estimate test error for ridge regression and lasso
set.seed(1)
train = sample(1:nrow(x), nrow(x) / 2) # random indexes
test = (-train)
y.test = y[test]
ridge.mod = glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,]) # predict with lambda = 4 and using test data
mean((ridge.pred - y.test)^2)
# predict with just intercept since lambda is very high and all coef are zero
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)
# Perform as least squares regressions, that it predict with lambda = 0
ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE)
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = TRUE, x = x[train, ], y = y[train], type = "coefficients")[1:20,]
## Use cross-validation to pick the best lambda
set.seed(1)
cv.out = cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2) # test MSE is lower than before with lambda = 4
# Refit ridge with full dataset and best lambda
out = glmnet(x, y, alpha = 0)
plot(out, xvar = "lambda")
predict(out, s = bestlam, type = "coefficients")[1:20,] # No coefficient is zero
lasso.mod = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef = predict(out, s = bestlam, type = "coefficients")[1:20,]
lasso.coef
lasso.coef[lasso.coef != 0]
## PCR and PLS
library(pls)
set.seed(2)
# standardize predictors and use 10-k fold CV
pcr.fit = pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit) # CV is root, MSE = CV ^ 2
validationplot(pcr.fit, val.type = "MSEP") # Plot CV MSE
names(pcr.fit)
set.seed(1)
pcr.fit = pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit, x[test, ], ncomp = 7) # M = 7 has lowest CV MSE
mean((pcr.pred - y.test)^2)
pcr.fit = pcr(y ~ x, scale = TRUE, ncomp = 7)
summary(pcr.fit)
# PLS
set.seed(1)
pls.fit = plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP") # Black line is CV, red line is adjCV
pls.pred = predict(pls.fit, x[test, ], ncomp = 2)
mean((pls.pred - y.test)^2)
validationplot(pls.fit, val.type = "MSEP") # Black line is CV, red line is adjCV
pls.pred = predict(pls.fit, x[test, ], ncomp = 2)
mean((pls.pred - y.test)^2)
pls.fit = plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 2)
summary(pls.fit)
rm(list = ls())
library(ISLR)
attach(Wage)
rm(list = ls())
attach(Wage)
fit = lm(wage ~ poly(age, 4), data = Wage)
?Wage
summary(fit)
agelims = range(age)
age.grid = seq(from = agelims[1], to = agelims[2])
preds = predict(fit, newdata = list(age = age.grid), se = TRUE) # Calculate also standard error
se.bands = cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree -4 Polynomial", outer = TRUE)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
fit.1 = lm(wage ~ poly(age, 1), data = Wage)
fit.2 = lm(wage ~ poly(age, 2), data = Wage)
fit.3 = lm(wage ~ poly(age, 3), data = Wage)
fit.4 = lm(wage ~ poly(age, 4), data = Wage)
fit.5 = lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5) # Models must be nested
# Predict if an individual earns > 250k per year
fit = glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial) # default prediction type is "link", not "response"
preds = predict(fit, newdata = list(age = age.grid), se = TRUE)
pfit = exp(preds$fit) / (1+exp(preds$fit))
se.bands.logit = cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands = exp(se.bands.logit) / (1+ exp(se.bands.logit))
plot(age, I(wage > 250), xlim = agelims, type = "n", ylim = c(0, .2))
points(jitter(age), I((wage > 250) / 5), cex = 5, pch = "|", col = "darkgrey")
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
# Step functions
table(cut(age, 4))
fit = lm(wage ~ cut(age, 4), data = Wage)
summary(fit) # Intercept is the first category, base average salary for people under 33.5 years
# Splines
library(splines)
# Create basis functions with given knots, by default are cubic splines
fit = lm(wage ~ bs(age, knots = c(25, 40, 50)), data = Wage)
pred = predict(fit, newdata = list(age = age.grid), se = TRUE)
plot(age, wage, col = "gray")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se.fit, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se.fit, lty = "dashed")
dim(bs(age, knots = c(25, 40, 60)))
dim(bs(age, df = 6))
attr(bs(age, df = 6), "knots")
# Natural spline
fit2 = lm(wage ~ ns(age, df = 4), data = Wage)
pred2 = predict(fit2, newdata = list(age = age.grid), se = TRUE)
lines(age.grid, pred2$fit, col = "red", lwd = 2)
# Smoothing spline
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Spline")
fit = smooth.spline(age, wage, df = 16)
fit2 = smooth.spline(age, wage, cv = TRUE) # Find df by using LOOCV
fit2$df # 6.8 df
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
# Local regression
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Local Regression")
fit = loess(wage ~ age, span = .2, data = Wage)
fit2 = loess(wage ~ age, span = .5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)), col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = "blue", lwd = 2)
legend("topright", legend = c("Span = 0.2", "Span = 0.5"), col = c("red", "blue"), lty = 1, lwd = 2, cex = 0.8)
# GAM
gam1 = lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
library(gam)
gam.m3 = gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) # use GAM with smooth spline
par(mfrow = c(3, 3))
plot(gam.m3, se = TRUE, col = "blue")
plot.Gam(gam1, se = TRUE, col = "red")
gam.m1 = gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 = gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
summary(gam.m3)
preds = predict(gam.m2, newdata = Wage)
gam.lo = gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage)
plot.Gam(gam.lo, se = TRUE, col = "green")
# local regression of the interaction between year and age
gam.lo.i = gam(wage ~ lo(year, age, span = 0.7) + education, data = Wage)
library(akima)
plot(gam.lo.i)
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, familh = binomial, data = Wage)
par(mfrow = c(1, 3))
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, familh = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
table(education, I(wage > 250))
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, familh = binomial, data = Wage)
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
table(education, I(wage > 250))
gam.lr.s = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage, subset = (education != "1. < HS Grad"))
plot(gam.lr.s, se = TRUE, col = "green")
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
table(education, I(wage > 250))
gam.lr.s = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage, subset = (education != "1. < HS Grad"))
plot(gam.lr.s, se = TRUE, col = "green")
# logistic regression in GAM
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
table(education, I(wage > 250))
gam.lr.s = gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage, subset = (education != "1. < HS Grad"))
plot(gam.lr.s, se = TRUE, col = "green")
data("cars")
cars
?cars
rm(list = ls())
cars
library(ISLR)
Cars93
Cars93
Cars93[, c("Price")]
Cars93[, c("Price", "MPG.city", "Horsepower", "Origin", "AirBags")]
View(Data)
Data = Cars93[, c("Price", "MPG.city", "Horsepower", "Origin", "AirBags")]
View(Data)
attach(Data)
contrasts(Origin)
contrasts(AirBags)
hist(Price)
hist(Price)
hist(log(Price))
Data$Price = log(Data$Price)
Price = log(Price)
hist(Price)
boxplot(Price)
boxplot(Cars93$Price)
boxplot(Price)
is.na(Price)
m1 = lm(Price ~ ., data = Data)
summary(m1)
?Cars93
plot(m1)
par(mfrow = c(2, 2))
plot(m1)
cor(Data)
?Data
View(Data)
cor(Data[, c("Price", "MPG.city")])
cor(Data[, c("Price", "MPG.city", "Origin")])
as.factor(Origin)
cor(cbind(Price, MPG.city))
cor(cbind(Price, MPG.city, as.factor(Origin)))
cor(cbind(Price, MPG.city, Horsepower, as.factor(Origin), as.factor(AirBags)))
m2 = lm(Price ~ MPG.city, data = Data)
summary(m2)
m2 = lm(Price ~ Horsepower, data = Data)
m3 = lm(Price ~ Horsepower, data = Data)
summary(m3)
m2 = lm(Price ~ MPG.city, data = Data)
summary(m2)
summary(m3)
m3 = lm(Price ~ MPG.city + Horsepower, data = Data)
summary(m3)
summary(m2)
summary(m3)
anova(m2, m3)
qt(0.9975)
qt(0.9975, 30)
qt(0.975, 30)
m4 = lm(Price ~ MPG.city + Horsepower + Origin, data = Data)
summary(m4)
anova(m3, m4)
m1 = lm(Price ~ ., data = Data)
summary(m1)
summary(Data)
View(Data)
Data[39, ]
dim(Data)
Data1 = Data[-39, ]
dim(Data1)
m5 = lm(Price ~ ., data = Data1)
summary(m5)
summary(m1)
cv.m3 = cv.glm(Data, m3, K = 10)
summary(cv.m3)
cv.m3$delta
cv.m3 = cv.glm(Data, m3, K = 10)
cv.m3$delta
library(boot)
cv.m3 = cv.glm(Data, m3, K = 10)
cv.m3$delta
cv.m3 = cv.glm(Data, m3)
cv.m3$delta
summary(m3)
Data
set.seed(123)
cv.m3 = cv.glm(Data, m3)
cv.m3$delta
summary(m3)
m3 = glm(Price ~ MPG.city + Horsepower, data = Data)
summary(m3)
m3 = lm(Price ~ MPG.city + Horsepower, data = Data)
summary(m3)
set.seed(123)
glm1 = glm(Price ~ MPG.city + Horsepower, data = Data)
cv.glm1 = cv.glm(Data, glm1)
cv.glm1$delta
set.seed(123)
glm1 = glm(Price ~ MPG.city + Horsepower, data = Data)
cv.glm1 = cv.glm(Data, glm1)
cv.glm1$delta
set.seed(123)
glm2 = glm(Price ~ MPG.city + Horsepower + Origin, data = Data)
cv.glm2 = cv.glm(Data, glm2)
cv.glm2$delta
summary(glm2)
set.seed(123)
glm1 = glm(Price ~ MPG.city + Horsepower  + Origin, data = Data)
cv.glm1 = cv.glm(Data, glm1)
cv.glm1$delta
set.seed(123)
glm2 = glm(Price ~ MPG.city + Horsepower + Origin + AirBags, data = Data)
cv.glm2 = cv.glm(Data, glm2)
cv.glm2$delta
pairs(Data)
pairs(Data[, 1:3])
m6 = lm(Price ~ poly(MPG.city, 2, raw = TRUE) + Horsepower + Origin, data = Data)
summary(m6)
anova(m1, m6)
summary(m1)
m1 = lm(Price ~ ., data = Data)
summary(m1)
m6 = lm(Price ~ poly(MPG.city, 2, raw = TRUE) + Horsepower + Origin + AirBags, data = Data)
summary(m6)
anova(m1, m6)
m7 = lm(Price ~ poly(MPG.city, 2, raw = TRUE) + poly(Horsepower, 2, raw = TRUE) + Origin + AirBags, data = Data)
summary(m7)
anova(m6, m7)
boxplot(Price ~ Origin)
boxplot(Price ~ Origin)
boxplot(Price ~ AirBags)
plot(Price ~ Horsepower, col = Origin)
plot(Price ~ MPG.city, col = Origin)
plot(Price ~ MPG.city, col = AirBags)
contrasts(AirBags)
plot(m6)
par(mfrow = c(2, 2))
plot(m6)
?anova
sp.mpg = smooth.spline(x = MPG.city, y = Price, cv = TRUE)
sp.mpg
sp.mpg = smooth.spline(x = Horsepower, y = Price, cv = TRUE)
sp.mpg
library(gam)
sp.mpg = smooth.spline(x = MPG.city, y = Price, cv = TRUE)
sp.mpg
sp.hp = smooth.spline(x = Horsepower, y = Price, cv = TRUE)
sp.hp
summary(m.gam)
m.gam = gam(Price ~ s(MPG.city, sp.mpg$df) + s(Horsepower, sp.hp$df) + Origin + AirBags, data = Data)
summary(m.gam)
m.gam2 = gam(Price ~ s(MPG.city, sp.mpg$df) + Horsepower + Origin + AirBags, data = Data)
summary(m.gam2)
anova(m.gam, m.gam2)
par(mfrow = c(1, 4))
plot(m.gam2, se = TRUE)
m.gam2 = gam(Price ~ s(MPG.city, 4) + Horsepower + Origin + AirBags, data = Data)
summary(m.gam2)
anova(m.gam, m.gam2)
par(mfrow = c(1, 4))
plot(m.gam2, se = TRUE)
par(mfrow = c(1, 2))
plot(Price, fitted(m6))
abline(0, 1, col = "red")
plot(Price, fitted(m.gam2))
abline(0, 1, col = "red")
library(glmnet)
m.lm = lm(Price ~ ., data = Cars93)
X = model.matrix(m.lm)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge)
m.ridge = glmnet(X, y, alpha = 0)
m.ridge = glmnet(x = X, y = y, alpha = 0)
X = model.matrix(Cars93)[, -1]
X = model.matrix(Price ~ ., Cars93)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
dim(y)
length(y)
dim(Cars93)
dim(X)
View(Cars93)
is.na(Cars93)
dim(na.omit(Cars93))
Cars93 = na.omit(Cars93)
m.lm = lm(Price ~ ., data = Cars93)
X = model.matrix(Price ~ ., Cars93)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge)
plot(m.ridge)
plot(m.ridge, xvar = "lambda")
Cars93 = na.omit(Cars93)
m.lm = lm(Price ~ ., data = Cars93)
X = model.matrix(m.lm)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge, xvar = "lambda")
set.seed(2906)
m.ridge.cv = cv.glmnet(X, y, alpha = 0)
plot(m.ridge.cv)
m.ridge.min = glmnet(X, y, alpha = 0, lambda = m.ridge.cv$lambda.min)
cbind(coef(m.lm), coef(m.ridge.min))
m.lm = lm(Cars$Price ~ ., data = Cars93)
X = model.matrix(m.lm)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge, xvar = "lambda")
rm(list = ls())
library(glmnet)
m.lm = lm(Cars$Price ~ ., data = Cars93)
m.lm = lm(Cars93$Price ~ ., data = Cars93)
X = model.matrix(m.lm)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge, xvar = "lambda")
m.ridge = glmnet(X, y, alpha = 0)
Cars93 = na.omit(Cars93)
m.lm = lm(Cars93$Price ~ ., data = Cars93)
X = model.matrix(m.lm)[, -1]
y = Cars93$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge, xvar = "lambda")
set.seed(2906)
m.ridge.cv = cv.glmnet(X, y, alpha = 0)
plot(m.ridge.cv)
m.ridge.min = glmnet(X, y, alpha = 0, lambda = m.ridge.cv$lambda.min)
cbind(coef(m.lm), coef(m.ridge.min))
summary(m.lm)
m.lm = lm(Cars93$Price ~ ., data = Cars93)
summary(m.lm)
Cars93
View(Cars93)
rm(list = ls())
setwd("~/Desktop/Repo/lab/R")
load("Cars.RData")
View(Cars)
View(cars)
m.lm = lm(cars$Price ~ ., data = cars)
X = model.matrix(m.lm)[, -1]
y = cars$Price
m.ridge = glmnet(X, y, alpha = 0)
plot(m.ridge, xvar = "lambda")
set.seed(2906)
m.ridge.cv = cv.glmnet(X, y, alpha = 0)
plot(m.ridge.cv)
m.ridge.min = glmnet(X, y, alpha = 0, lambda = m.ridge.cv$lambda.min)
cbind(coef(m.lm), coef(m.ridge.min))
m.lasso = glmnet(X, y, alpha = 1)
plot(m.lasso, xvar = "lambda")
set.seed(2906)
m.lasso.cv = cv.glmnet(X, y, alpha = 1)
plot(m.lasso.cv)
m.lasso.min = glmnet(X, y, alpha = 1, lambda = m.lasso.cv$lambda.min)
cbind(coef(m.lm), coef(m.ridge.min), coef(m.lasso.min))
par(mfrow = c(1, 2))
plot(predict(m.ridge.min, newx = X), y)
abline(0, 1, col = "red")
plot(predict(m.lasso.min, newx = X), y)
abline(0, 1, col = "red")
min(m.ridge.cv$cvm)
min(.lasso.cv$cvm)
min(m.lasso.cv$cvm)
m.lasso.min = glmnet(X, y, alpha = 1, lambda = m.lasso.cv$lambda.1se)
cbind(coef(m.lm), coef(m.ridge.min), coef(m.lasso.min))
par(mfrow = c(1, 2))
plot(predict(m.ridge.min, newx = X), y)
abline(0, 1, col = "red")
plot(predict(m.lasso.min, newx = X), y)
abline(0, 1, col = "red")
min(m.ridge.cv$cvm)
min(m.lasso.cv$cvm)
coef(m.lasso.min) != 0
subset(coef(m.lasso.min), coef(m.lasso.min) != 0)
subset(coef(m.lasso.min), slect = coef(m.lasso.min) != 0)
subset(coef(m.lasso.min), select = coef(m.lasso.min) != 0)
coef(m.lasso.min)[coef(m.lasso.min) != 0]
id.nonzero = which(coef(m.lasso.min) != 0)
varnames = rownames(coef(m.lasso.min))[id.nonzero]
values = coef(m.lasso.min)[id.nonzero]
names(values) = varnames
values
varnames
dim(cars)
?leaps
library(leaps)
m.forward = regsubsets(cars$Price ~ ., data = cars, nvmax = 22, method = "seqrep")
summary(m.forward)
plot(m.forward)
plot(m.forward)
which.min(summary(m.forward)$bic)
which.min(summary(m.forward)$adjr2)
which.min(summary(m.forward)$r2)
names(summary(m.forward))
which.min(summary(m.forward)$rsq)
which.min(summary(m.forward)$adjr2)
which.min(summary(m.forward)$cp)
which.min(summary(m.forward)$bic)
coef(m.forward, 8)
values
coef(m.forward, 8)
values
par(mfrow = c(1, 2))
plot(predict(m.forward, newx = X), y)
m.forward = regsubsets(cars$Price ~ ., data = cars, nvmax = 22, method = "forward")
which.min(summary(m.forward)$bic)
coef(m.forward, 8)
values
